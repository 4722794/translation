num_epochs: 20
vocab_source: 5001
vocab_target: 5001
optimizer: adamw
scheduler: cosine
dropout: 0.4
hidden_size: 512 
embedding_size: 128
learning_rate: 0.001
batch_size: 512
num_layers: 1
dot_product: false